{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Aj5JKdkdvWhP"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed May 22 08:37:24 2024\n",
    "\n",
    "@author: jaimeunriza\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import r2_score, roc_auc_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Dropout\n",
    "import keras.optimizers as opts\n",
    "from keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lYLkpK6PvcQa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/robertogb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/robertogb/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/robertogb/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Descargar los recursos necesarios de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UvV6dUspvffZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Carga de datos de archivo .csv\n",
    "dataTraining = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', encoding='UTF-8', index_col=0)\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', encoding='UTF-8', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>plot</th>\n",
       "      <th>genres</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3107</th>\n",
       "      <td>2003</td>\n",
       "      <td>Most</td>\n",
       "      <td>most is the story of a single father who takes...</td>\n",
       "      <td>['Short', 'Drama']</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>2008</td>\n",
       "      <td>How to Be a Serial Killer</td>\n",
       "      <td>a serial killer decides to teach the secrets o...</td>\n",
       "      <td>['Comedy', 'Crime', 'Horror']</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6724</th>\n",
       "      <td>1941</td>\n",
       "      <td>A Woman's Face</td>\n",
       "      <td>in sweden ,  a female blackmailer with a disfi...</td>\n",
       "      <td>['Drama', 'Film-Noir', 'Thriller']</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4704</th>\n",
       "      <td>1954</td>\n",
       "      <td>Executive Suite</td>\n",
       "      <td>in a friday afternoon in new york ,  the presi...</td>\n",
       "      <td>['Drama']</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2582</th>\n",
       "      <td>1990</td>\n",
       "      <td>Narrow Margin</td>\n",
       "      <td>in los angeles ,  the editor of a publishing h...</td>\n",
       "      <td>['Action', 'Crime', 'Thriller']</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      year                      title  \\\n",
       "3107  2003                       Most   \n",
       "900   2008  How to Be a Serial Killer   \n",
       "6724  1941             A Woman's Face   \n",
       "4704  1954            Executive Suite   \n",
       "2582  1990              Narrow Margin   \n",
       "\n",
       "                                                   plot  \\\n",
       "3107  most is the story of a single father who takes...   \n",
       "900   a serial killer decides to teach the secrets o...   \n",
       "6724  in sweden ,  a female blackmailer with a disfi...   \n",
       "4704  in a friday afternoon in new york ,  the presi...   \n",
       "2582  in los angeles ,  the editor of a publishing h...   \n",
       "\n",
       "                                  genres  rating  \n",
       "3107                  ['Short', 'Drama']     8.0  \n",
       "900        ['Comedy', 'Crime', 'Horror']     5.6  \n",
       "6724  ['Drama', 'Film-Noir', 'Thriller']     7.2  \n",
       "4704                           ['Drama']     7.4  \n",
       "2582     ['Action', 'Crime', 'Thriller']     6.6  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataTraining.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7895, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataTraining.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>plot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999</td>\n",
       "      <td>Message in a Bottle</td>\n",
       "      <td>who meets by fate ,  shall be sealed by fate ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1978</td>\n",
       "      <td>Midnight Express</td>\n",
       "      <td>the true story of billy hayes ,  an american c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1996</td>\n",
       "      <td>Primal Fear</td>\n",
       "      <td>martin vail left the chicago da ' s office to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1950</td>\n",
       "      <td>Crisis</td>\n",
       "      <td>husband and wife americans dr .  eugene and mr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1959</td>\n",
       "      <td>The Tingler</td>\n",
       "      <td>the coroner and scientist dr .  warren chapin ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year                title  \\\n",
       "1  1999  Message in a Bottle   \n",
       "4  1978     Midnight Express   \n",
       "5  1996          Primal Fear   \n",
       "6  1950               Crisis   \n",
       "7  1959          The Tingler   \n",
       "\n",
       "                                                plot  \n",
       "1  who meets by fate ,  shall be sealed by fate ....  \n",
       "4  the true story of billy hayes ,  an american c...  \n",
       "5  martin vail left the chicago da ' s office to ...  \n",
       "6  husband and wife americans dr .  eugene and mr...  \n",
       "7  the coroner and scientist dr .  warren chapin ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataTesting.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3383, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataTesting.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vEN2oSR_vinJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DTM: (7895, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Función para lematizar palabras con su respectiva POS\n",
    "def lemmatize_with_pos(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Tokenizar las palabras\n",
    "    tokens = word_tokenize(text)\n",
    "    # Etiquetar POS de cada palabra\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    # Lematizar cada palabra según su POS\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word, pos=pos_tag[0].lower())\n",
    "                         if pos_tag[0].lower() in ['n', 'v', 'a', 'r'] else word\n",
    "                         for word, pos_tag in tagged_tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Concatenar título y año con la trama\n",
    "dataTraining['plot_title_year'] =  dataTraining['plot'] + ' ' + dataTraining['title'] + ' ' + dataTraining['year'].astype(str)\n",
    "dataTesting['plot_title_year'] =  dataTesting['plot'] + ' ' + dataTesting['title'] + ' ' + dataTesting['year'].astype(str)\n",
    "\n",
    "# Aplicar lematización con POS a los datos de entrenamiento\n",
    "X_train_lemmatized = [lemmatize_with_pos(text) for text in dataTraining['plot_title_year']]\n",
    "\n",
    "\n",
    "# Crear el vectorizador CountVectorizer\n",
    "vect = CountVectorizer(lowercase=True, strip_accents='ascii', stop_words='english',ngram_range=(1,2), max_features=10000)\n",
    "\n",
    "# Definición de variables predictoras (X)\n",
    "X_dtm = vect.fit_transform(X_train_lemmatized)\n",
    "print(\"Shape of DTM:\", X_dtm.shape)\n",
    "\n",
    "# Definición de variable de interés (y)\n",
    "dataTraining['genres'] = dataTraining['genres'].map(lambda x: eval(x))\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_genres = mlb.fit_transform(dataTraining['genres'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Tbx4SklGvwFd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 input variables\n",
      "24  output variables\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Separación de variables predictoras (X) y variable de interés (y) en set de entrenamiento y test usandola función train_test_split\n",
    "X_train, X_test, y_train_genres, y_test_genres = train_test_split(X_dtm, y_genres, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "# Definición de dimensiones de entrada, varaibles predictoras\n",
    "dims = X_train.shape[1]\n",
    "print(dims, 'input variables')\n",
    "\n",
    "# Definición de dimensiones de salida, varaibles de interés\n",
    "output_var = y_train_genres.shape[1]\n",
    "print(output_var, ' output variables')\n",
    "\n",
    "######################### modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Radom Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score (Random Forest): 0.5545072093765353\n"
     ]
    }
   ],
   "source": [
    "rf_model = OneVsRestClassifier(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "rf_model.fit(X_train, y_train_genres)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "roc_auc_rf = roc_auc_score(y_test_genres, y_pred_rf, average='macro')\n",
    "print(\"ROC AUC Score (Random Forest):\", roc_auc_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score (Logistic Regression): 0.6538607677270819\n"
     ]
    }
   ],
   "source": [
    "lr_model = OneVsRestClassifier(LogisticRegression(max_iter=1000, random_state=42))\n",
    "lr_model.fit(X_train, y_train_genres)\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "roc_auc_lr = roc_auc_score(y_test_genres, y_pred_lr, average='macro')\n",
    "print(\"ROC AUC Score (Logistic Regression):\", roc_auc_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "j76jxRjuvoEK"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3500</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">35,003,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3500</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">84,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3500\u001b[0m)           │    \u001b[38;5;34m35,003,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3500\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │        \u001b[38;5;34m84,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">35,087,524</span> (133.85 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m35,087,524\u001b[0m (133.85 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">35,087,524</span> (133.85 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m35,087,524\u001b[0m (133.85 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/5\n",
      "\u001b[1m632/632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 201ms/step - loss: 0.1204 - val_loss: 0.0988\n",
      "Epoch 2/5\n",
      "\u001b[1m632/632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 259ms/step - loss: 0.0966 - val_loss: 0.0912\n",
      "Epoch 3/5\n",
      "\u001b[1m632/632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 261ms/step - loss: 0.0872 - val_loss: 0.0869\n",
      "Epoch 4/5\n",
      "\u001b[1m632/632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 255ms/step - loss: 0.0807 - val_loss: 0.0841\n",
      "Epoch 5/5\n",
      "\u001b[1m632/632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 428ms/step - loss: 0.0767 - val_loss: 0.0821\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "ROC AUC Score (Red Neuronal): 0.7493899317868015\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "# Definición red neuronal con la función Sequential()\n",
    "model = Sequential()\n",
    "\n",
    "# Definición de la capa densa con un tamaño de salida igual a output_var y un input_shape de dims\n",
    "model.add(Dense(3500, input_shape=(dims,),activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(Dense(1500,activation='tanh'))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Dense(output_var))\n",
    "model.add(Activation('tanh'))\n",
    "\n",
    "\n",
    "# Impresión de la arquitectura de la red neuronal\n",
    "print(model.summary())\n",
    "\n",
    "# Definición de función de perdida. Se usa mean_squared_error dado que es un ejercicio de regresión\n",
    "from keras.optimizers import SGD,Adam, RMSprop, Adagrad, Adadelta, Adamax, Nadam\n",
    "model.compile(optimizer=Adadelta(learning_rate=0.020), loss='mean_squared_error')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Ordenar los índices de la matriz dispersa X_train\n",
    "X_train.sort_indices()\n",
    "\n",
    "# Similarmente, ordenar los índices de la matriz dispersa X_test si es necesario\n",
    "X_test.sort_indices()\n",
    "\n",
    "# Definición de la función EarlyStopping con parámetro definido en la función nn_model_params\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience = 5)\n",
    "\n",
    "# Entrenamiento de la red neuronal con ## épocas\n",
    "model.fit(X_train, y_train_genres,batch_size=10,\n",
    "          validation_data = (X_test, y_test_genres),\n",
    "          epochs=5,\n",
    "          callbacks=[early_stopping])\n",
    "\n",
    "# Predicción del modelo\n",
    "y_pred_nn = model.predict(X_test)\n",
    "\n",
    "# Calcula el área bajo la curva ROC (ROC AUC)\n",
    "roc_auc_nn = roc_auc_score(y_test_genres, y_pred_nn, average='macro')\n",
    "\n",
    "# Imprime el desempeño del modelo\n",
    "print(\"ROC AUC Score (Red Neuronal):\", roc_auc_nn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparación de Modelos\n",
      "Red Neuronal: ROC AUC = 0.7493899317868015\n",
      "Random Forest: ROC AUC = 0.5545072093765353\n",
      "Logistic Regression: ROC AUC = 0.6538607677270819\n"
     ]
    }
   ],
   "source": [
    "# Comparación de resultados\n",
    "print(\"\\nComparación de Modelos\")\n",
    "print(f\"Red Neuronal: ROC AUC = {roc_auc_nn}\")\n",
    "print(f\"Random Forest: ROC AUC = {roc_auc_rf}\")\n",
    "print(f\"Logistic Regression: ROC AUC = {roc_auc_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección Mejor Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Analizando los resultados de las métricas de desempeño, en este caso y específicamente el ROC-AUC (Área bajo la curva) para cada modelo. El ROC-AUC es una medida de la capacidad de un modelo para distinguir entre clases. El valor más alto indica un mejor rendimiento.\n",
    "\n",
    "**Resultados de los Modelos**\n",
    "- Red Neuronal: ROC AUC = 0.7494\n",
    "- Random Forest: ROC AUC = 0.5545\n",
    "- Logistic Regression: ROC AUC = 0.6539\n",
    "\n",
    "**Mejor Modelo Red Neuronal (ROC AUC = 0.7494)**\n",
    "Indica que la red neuronal tiene una buena capacidad para predecir la probabilidad de que una película pertenezca a un género en particular. Si bien es importante considerar el factor tiempo de entrenamiento, la complejidad del modelo y la interpretabilidad, el modelo Red Neuronal permite obtener un mejor desempeño significativamnte superior a los otros dos modelos Radom Forest y Logistic Regression.\n",
    "\n",
    "Los resultados obtenidos se pueden considerar como esperables en el caso de problemas de NLP (Procesamiento de Lenguaje Natural), las redes neuronales y esquemas LSTM nos demuestran que son muy efectivas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción Mejor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "OTetLP-5v2Ng"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_Action</th>\n",
       "      <th>p_Adventure</th>\n",
       "      <th>p_Animation</th>\n",
       "      <th>p_Biography</th>\n",
       "      <th>p_Comedy</th>\n",
       "      <th>p_Crime</th>\n",
       "      <th>p_Documentary</th>\n",
       "      <th>p_Drama</th>\n",
       "      <th>p_Family</th>\n",
       "      <th>p_Fantasy</th>\n",
       "      <th>...</th>\n",
       "      <th>p_Musical</th>\n",
       "      <th>p_Mystery</th>\n",
       "      <th>p_News</th>\n",
       "      <th>p_Romance</th>\n",
       "      <th>p_Sci-Fi</th>\n",
       "      <th>p_Short</th>\n",
       "      <th>p_Sport</th>\n",
       "      <th>p_Thriller</th>\n",
       "      <th>p_War</th>\n",
       "      <th>p_Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.129089</td>\n",
       "      <td>0.184302</td>\n",
       "      <td>0.060074</td>\n",
       "      <td>0.051592</td>\n",
       "      <td>0.399331</td>\n",
       "      <td>0.090644</td>\n",
       "      <td>0.024790</td>\n",
       "      <td>0.454759</td>\n",
       "      <td>-0.125685</td>\n",
       "      <td>0.091039</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027958</td>\n",
       "      <td>0.147027</td>\n",
       "      <td>0.058980</td>\n",
       "      <td>0.321345</td>\n",
       "      <td>-0.012792</td>\n",
       "      <td>0.018856</td>\n",
       "      <td>0.013376</td>\n",
       "      <td>0.076395</td>\n",
       "      <td>-0.090675</td>\n",
       "      <td>-0.019367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.098633</td>\n",
       "      <td>-0.137732</td>\n",
       "      <td>-0.019283</td>\n",
       "      <td>0.183989</td>\n",
       "      <td>0.208818</td>\n",
       "      <td>0.100744</td>\n",
       "      <td>0.005444</td>\n",
       "      <td>0.514810</td>\n",
       "      <td>-0.004787</td>\n",
       "      <td>0.044391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108273</td>\n",
       "      <td>0.122407</td>\n",
       "      <td>-0.010430</td>\n",
       "      <td>0.069012</td>\n",
       "      <td>-0.086326</td>\n",
       "      <td>0.044232</td>\n",
       "      <td>0.001842</td>\n",
       "      <td>0.181614</td>\n",
       "      <td>0.089033</td>\n",
       "      <td>0.047333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.173774</td>\n",
       "      <td>0.009113</td>\n",
       "      <td>0.040335</td>\n",
       "      <td>0.080175</td>\n",
       "      <td>-0.155478</td>\n",
       "      <td>0.687660</td>\n",
       "      <td>-0.088260</td>\n",
       "      <td>0.841854</td>\n",
       "      <td>-0.195753</td>\n",
       "      <td>-0.203197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032964</td>\n",
       "      <td>0.168047</td>\n",
       "      <td>-0.272761</td>\n",
       "      <td>0.225255</td>\n",
       "      <td>0.187306</td>\n",
       "      <td>0.011013</td>\n",
       "      <td>0.092102</td>\n",
       "      <td>0.851831</td>\n",
       "      <td>-0.550483</td>\n",
       "      <td>-0.136624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.136025</td>\n",
       "      <td>0.094897</td>\n",
       "      <td>0.010886</td>\n",
       "      <td>0.165577</td>\n",
       "      <td>0.209038</td>\n",
       "      <td>0.104102</td>\n",
       "      <td>0.187370</td>\n",
       "      <td>0.672141</td>\n",
       "      <td>0.111046</td>\n",
       "      <td>-0.046760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.112807</td>\n",
       "      <td>0.111829</td>\n",
       "      <td>0.050792</td>\n",
       "      <td>-0.042713</td>\n",
       "      <td>0.068157</td>\n",
       "      <td>0.255437</td>\n",
       "      <td>0.237588</td>\n",
       "      <td>0.418159</td>\n",
       "      <td>0.066017</td>\n",
       "      <td>-0.022192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.079627</td>\n",
       "      <td>-0.201107</td>\n",
       "      <td>-0.088293</td>\n",
       "      <td>-0.135205</td>\n",
       "      <td>0.420597</td>\n",
       "      <td>0.209097</td>\n",
       "      <td>-0.037694</td>\n",
       "      <td>-0.274813</td>\n",
       "      <td>-0.221566</td>\n",
       "      <td>0.285618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128819</td>\n",
       "      <td>0.202090</td>\n",
       "      <td>-0.022768</td>\n",
       "      <td>0.115525</td>\n",
       "      <td>0.511115</td>\n",
       "      <td>0.022251</td>\n",
       "      <td>-0.128303</td>\n",
       "      <td>0.197575</td>\n",
       "      <td>-0.135037</td>\n",
       "      <td>-0.224616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   p_Action  p_Adventure  p_Animation  p_Biography  p_Comedy   p_Crime  \\\n",
       "1  0.129089     0.184302     0.060074     0.051592  0.399331  0.090644   \n",
       "4  0.098633    -0.137732    -0.019283     0.183989  0.208818  0.100744   \n",
       "5 -0.173774     0.009113     0.040335     0.080175 -0.155478  0.687660   \n",
       "6  0.136025     0.094897     0.010886     0.165577  0.209038  0.104102   \n",
       "7 -0.079627    -0.201107    -0.088293    -0.135205  0.420597  0.209097   \n",
       "\n",
       "   p_Documentary   p_Drama  p_Family  p_Fantasy  ...  p_Musical  p_Mystery  \\\n",
       "1       0.024790  0.454759 -0.125685   0.091039  ...  -0.027958   0.147027   \n",
       "4       0.005444  0.514810 -0.004787   0.044391  ...   0.108273   0.122407   \n",
       "5      -0.088260  0.841854 -0.195753  -0.203197  ...   0.032964   0.168047   \n",
       "6       0.187370  0.672141  0.111046  -0.046760  ...  -0.112807   0.111829   \n",
       "7      -0.037694 -0.274813 -0.221566   0.285618  ...   0.128819   0.202090   \n",
       "\n",
       "     p_News  p_Romance  p_Sci-Fi   p_Short   p_Sport  p_Thriller     p_War  \\\n",
       "1  0.058980   0.321345 -0.012792  0.018856  0.013376    0.076395 -0.090675   \n",
       "4 -0.010430   0.069012 -0.086326  0.044232  0.001842    0.181614  0.089033   \n",
       "5 -0.272761   0.225255  0.187306  0.011013  0.092102    0.851831 -0.550483   \n",
       "6  0.050792  -0.042713  0.068157  0.255437  0.237588    0.418159  0.066017   \n",
       "7 -0.022768   0.115525  0.511115  0.022251 -0.128303    0.197575 -0.135037   \n",
       "\n",
       "   p_Western  \n",
       "1  -0.019367  \n",
       "4   0.047333  \n",
       "5  -0.136624  \n",
       "6  -0.022192  \n",
       "7  -0.224616  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicción del conjunto de test utilizando el mejor modelo (Red Neuronal)\n",
    "\n",
    "# transformación variables predictoras X del conjunto de test\n",
    "X_test_dtm = vect.transform(dataTesting[  'plot_title_year'   ])\n",
    "\n",
    "cols = ['p_Action', 'p_Adventure', 'p_Animation', 'p_Biography', 'p_Comedy', 'p_Crime', 'p_Documentary', 'p_Drama', 'p_Family',\n",
    "        'p_Fantasy', 'p_Film-Noir', 'p_History', 'p_Horror', 'p_Music', 'p_Musical', 'p_Mystery', 'p_News', 'p_Romance',\n",
    "        'p_Sci-Fi', 'p_Short', 'p_Sport', 'p_Thriller', 'p_War', 'p_Western']\n",
    "\n",
    "\n",
    "# Predicción del conjunto de test\n",
    "y_pred_proba = model.predict(X_test_dtm)\n",
    "\n",
    "\n",
    "\n",
    "# Guardar predicciones en formato exigido en la competencia de kaggle\n",
    "res = pd.DataFrame(y_pred_proba, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_text_RN11.csv', index_label='ID')\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear API con Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Guardar el modelo entrenado\n",
    "model.save('model.h5')\n",
    "\n",
    "import pickle\n",
    "# Guardar el vectorizador\n",
    "with open('vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vect, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Cargar el modelo y el vectorizador\n",
    "model = load_model('model.h5')\n",
    "with open('vectorizer.pkl', 'rb') as f:\n",
    "    vect = pickle.load(f)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.json\n",
    "    plot_title_year = data['plot_title_year']\n",
    "    \n",
    "    # Preprocesamiento\n",
    "    X = vect.transform([plot_title_year])\n",
    "    \n",
    "    # Predicción\n",
    "    prediction = model.predict(X)\n",
    "    \n",
    "    # Crear respuesta\n",
    "    response = {\n",
    "        'predictions': prediction.tolist()\n",
    "    }\n",
    "    return jsonify(response)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutar la API localmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecución de la aplicación que disponibiliza el modelo de manera local en el puerto 5000\n",
    "app.run(debug=True, use_reloader=False, host='0.0.0.0', port=5001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
